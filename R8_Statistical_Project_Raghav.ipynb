{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Riknp3h6V-D8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "FBHzhZp5X-OX",
    "outputId": "38f89912-ec00-401c-99cc-cebea4a6a0b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hEgyFcndYL_I"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/My Drive/nltk_project/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dO1DYR4zYeLE"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "with ZipFile('blog-authorship-corpus.zip', 'r') as z:\n",
    "  z.extractall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-0XdD_WuYqTG",
    "outputId": "d8032cfa-2e0b-48fd-a2eb-23ceeb754d12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blog-authorship-corpus.zip  blogtext.csv\n"
     ]
    }
   ],
   "source": [
    "ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "pGKDyjIIiwRu",
    "outputId": "471205a0-bbda-4bdf-b0e4-06cebcc2fa8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> all\n",
      "    Downloading collection 'all'\n",
      "       | \n",
      "       | Downloading package abc to /root/nltk_data...\n",
      "       |   Unzipping corpora/abc.zip.\n",
      "       | Downloading package alpino to /root/nltk_data...\n",
      "       |   Unzipping corpora/alpino.zip.\n",
      "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
      "       |   Unzipping corpora/biocreative_ppi.zip.\n",
      "       | Downloading package brown to /root/nltk_data...\n",
      "       |   Unzipping corpora/brown.zip.\n",
      "       | Downloading package brown_tei to /root/nltk_data...\n",
      "       |   Unzipping corpora/brown_tei.zip.\n",
      "       | Downloading package cess_cat to /root/nltk_data...\n",
      "       |   Unzipping corpora/cess_cat.zip.\n",
      "       | Downloading package cess_esp to /root/nltk_data...\n",
      "       |   Unzipping corpora/cess_esp.zip.\n",
      "       | Downloading package chat80 to /root/nltk_data...\n",
      "       |   Unzipping corpora/chat80.zip.\n",
      "       | Downloading package city_database to /root/nltk_data...\n",
      "       |   Unzipping corpora/city_database.zip.\n",
      "       | Downloading package cmudict to /root/nltk_data...\n",
      "       |   Unzipping corpora/cmudict.zip.\n",
      "       | Downloading package comparative_sentences to\n",
      "       |     /root/nltk_data...\n",
      "       |   Unzipping corpora/comparative_sentences.zip.\n",
      "       | Downloading package comtrans to /root/nltk_data...\n",
      "       | Downloading package conll2000 to /root/nltk_data...\n",
      "       |   Unzipping corpora/conll2000.zip.\n",
      "       | Downloading package conll2002 to /root/nltk_data...\n",
      "       |   Unzipping corpora/conll2002.zip.\n",
      "       | Downloading package conll2007 to /root/nltk_data...\n",
      "       | Downloading package crubadan to /root/nltk_data...\n",
      "       |   Unzipping corpora/crubadan.zip.\n",
      "       | Downloading package dependency_treebank to /root/nltk_data...\n",
      "       |   Unzipping corpora/dependency_treebank.zip.\n",
      "       | Downloading package dolch to /root/nltk_data...\n",
      "       |   Unzipping corpora/dolch.zip.\n",
      "       | Downloading package europarl_raw to /root/nltk_data...\n",
      "       |   Unzipping corpora/europarl_raw.zip.\n",
      "       | Downloading package floresta to /root/nltk_data...\n",
      "       |   Unzipping corpora/floresta.zip.\n",
      "       | Downloading package framenet_v15 to /root/nltk_data...\n",
      "       |   Unzipping corpora/framenet_v15.zip.\n",
      "       | Downloading package framenet_v17 to /root/nltk_data...\n",
      "       |   Unzipping corpora/framenet_v17.zip.\n",
      "       | Downloading package gazetteers to /root/nltk_data...\n",
      "       |   Unzipping corpora/gazetteers.zip.\n",
      "       | Downloading package genesis to /root/nltk_data...\n",
      "       |   Unzipping corpora/genesis.zip.\n",
      "       | Downloading package gutenberg to /root/nltk_data...\n",
      "       |   Unzipping corpora/gutenberg.zip.\n",
      "       | Downloading package ieer to /root/nltk_data...\n",
      "       |   Unzipping corpora/ieer.zip.\n",
      "       | Downloading package inaugural to /root/nltk_data...\n",
      "       |   Unzipping corpora/inaugural.zip.\n",
      "       | Downloading package indian to /root/nltk_data...\n",
      "       |   Unzipping corpora/indian.zip.\n",
      "       | Downloading package jeita to /root/nltk_data...\n",
      "       | Downloading package kimmo to /root/nltk_data...\n",
      "       |   Unzipping corpora/kimmo.zip.\n",
      "       | Downloading package knbc to /root/nltk_data...\n",
      "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
      "       |   Unzipping corpora/lin_thesaurus.zip.\n",
      "       | Downloading package mac_morpho to /root/nltk_data...\n",
      "       |   Unzipping corpora/mac_morpho.zip.\n",
      "       | Downloading package machado to /root/nltk_data...\n",
      "       | Downloading package masc_tagged to /root/nltk_data...\n",
      "       | Downloading package moses_sample to /root/nltk_data...\n",
      "       |   Unzipping models/moses_sample.zip.\n",
      "       | Downloading package movie_reviews to /root/nltk_data...\n",
      "       |   Unzipping corpora/movie_reviews.zip.\n",
      "       | Downloading package names to /root/nltk_data...\n",
      "       |   Unzipping corpora/names.zip.\n",
      "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
      "       | Downloading package nps_chat to /root/nltk_data...\n",
      "       |   Unzipping corpora/nps_chat.zip.\n",
      "       | Downloading package omw to /root/nltk_data...\n",
      "       |   Unzipping corpora/omw.zip.\n",
      "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
      "       |   Unzipping corpora/opinion_lexicon.zip.\n",
      "       | Downloading package paradigms to /root/nltk_data...\n",
      "       |   Unzipping corpora/paradigms.zip.\n",
      "       | Downloading package pil to /root/nltk_data...\n",
      "       |   Unzipping corpora/pil.zip.\n",
      "       | Downloading package pl196x to /root/nltk_data...\n",
      "       |   Unzipping corpora/pl196x.zip.\n",
      "       | Downloading package ppattach to /root/nltk_data...\n",
      "       |   Unzipping corpora/ppattach.zip.\n",
      "       | Downloading package problem_reports to /root/nltk_data...\n",
      "       |   Unzipping corpora/problem_reports.zip.\n",
      "       | Downloading package propbank to /root/nltk_data...\n",
      "       | Downloading package ptb to /root/nltk_data...\n",
      "       |   Unzipping corpora/ptb.zip.\n",
      "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
      "       |   Unzipping corpora/product_reviews_1.zip.\n",
      "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
      "       |   Unzipping corpora/product_reviews_2.zip.\n",
      "       | Downloading package pros_cons to /root/nltk_data...\n",
      "       |   Unzipping corpora/pros_cons.zip.\n",
      "       | Downloading package qc to /root/nltk_data...\n",
      "       |   Unzipping corpora/qc.zip.\n",
      "       | Downloading package reuters to /root/nltk_data...\n",
      "       | Downloading package rte to /root/nltk_data...\n",
      "       |   Unzipping corpora/rte.zip.\n",
      "       | Downloading package semcor to /root/nltk_data...\n",
      "       | Downloading package senseval to /root/nltk_data...\n",
      "       |   Unzipping corpora/senseval.zip.\n",
      "       | Downloading package sentiwordnet to /root/nltk_data...\n",
      "       |   Unzipping corpora/sentiwordnet.zip.\n",
      "       | Downloading package sentence_polarity to /root/nltk_data...\n",
      "       |   Unzipping corpora/sentence_polarity.zip.\n",
      "       | Downloading package shakespeare to /root/nltk_data...\n",
      "       |   Unzipping corpora/shakespeare.zip.\n",
      "       | Downloading package sinica_treebank to /root/nltk_data...\n",
      "       |   Unzipping corpora/sinica_treebank.zip.\n",
      "       | Downloading package smultron to /root/nltk_data...\n",
      "       |   Unzipping corpora/smultron.zip.\n",
      "       | Downloading package state_union to /root/nltk_data...\n",
      "       |   Unzipping corpora/state_union.zip.\n",
      "       | Downloading package stopwords to /root/nltk_data...\n",
      "       |   Unzipping corpora/stopwords.zip.\n",
      "       | Downloading package subjectivity to /root/nltk_data...\n",
      "       |   Unzipping corpora/subjectivity.zip.\n",
      "       | Downloading package swadesh to /root/nltk_data...\n",
      "       |   Unzipping corpora/swadesh.zip.\n",
      "       | Downloading package switchboard to /root/nltk_data...\n",
      "       |   Unzipping corpora/switchboard.zip.\n",
      "       | Downloading package timit to /root/nltk_data...\n",
      "       |   Unzipping corpora/timit.zip.\n",
      "       | Downloading package toolbox to /root/nltk_data...\n",
      "       |   Unzipping corpora/toolbox.zip.\n",
      "       | Downloading package treebank to /root/nltk_data...\n",
      "       |   Unzipping corpora/treebank.zip.\n",
      "       | Downloading package twitter_samples to /root/nltk_data...\n",
      "       |   Unzipping corpora/twitter_samples.zip.\n",
      "       | Downloading package udhr to /root/nltk_data...\n",
      "       |   Unzipping corpora/udhr.zip.\n",
      "       | Downloading package udhr2 to /root/nltk_data...\n",
      "       |   Unzipping corpora/udhr2.zip.\n",
      "       | Downloading package unicode_samples to /root/nltk_data...\n",
      "       |   Unzipping corpora/unicode_samples.zip.\n",
      "       | Downloading package universal_treebanks_v20 to\n",
      "       |     /root/nltk_data...\n",
      "       | Downloading package verbnet to /root/nltk_data...\n",
      "       |   Unzipping corpora/verbnet.zip.\n",
      "       | Downloading package verbnet3 to /root/nltk_data...\n",
      "       |   Unzipping corpora/verbnet3.zip.\n",
      "       | Downloading package webtext to /root/nltk_data...\n",
      "       |   Unzipping corpora/webtext.zip.\n",
      "       | Downloading package wordnet to /root/nltk_data...\n",
      "       |   Unzipping corpora/wordnet.zip.\n",
      "       | Downloading package wordnet_ic to /root/nltk_data...\n",
      "       |   Unzipping corpora/wordnet_ic.zip.\n",
      "       | Downloading package words to /root/nltk_data...\n",
      "       |   Unzipping corpora/words.zip.\n",
      "       | Downloading package ycoe to /root/nltk_data...\n",
      "       |   Unzipping corpora/ycoe.zip.\n",
      "       | Downloading package rslp to /root/nltk_data...\n",
      "       |   Unzipping stemmers/rslp.zip.\n",
      "       | Downloading package maxent_treebank_pos_tagger to\n",
      "       |     /root/nltk_data...\n",
      "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "       | Downloading package universal_tagset to /root/nltk_data...\n",
      "       |   Unzipping taggers/universal_tagset.zip.\n",
      "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
      "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "       | Downloading package punkt to /root/nltk_data...\n",
      "       |   Unzipping tokenizers/punkt.zip.\n",
      "       | Downloading package book_grammars to /root/nltk_data...\n",
      "       |   Unzipping grammars/book_grammars.zip.\n",
      "       | Downloading package sample_grammars to /root/nltk_data...\n",
      "       |   Unzipping grammars/sample_grammars.zip.\n",
      "       | Downloading package spanish_grammars to /root/nltk_data...\n",
      "       |   Unzipping grammars/spanish_grammars.zip.\n",
      "       | Downloading package basque_grammars to /root/nltk_data...\n",
      "       |   Unzipping grammars/basque_grammars.zip.\n",
      "       | Downloading package large_grammars to /root/nltk_data...\n",
      "       |   Unzipping grammars/large_grammars.zip.\n",
      "       | Downloading package tagsets to /root/nltk_data...\n",
      "       |   Unzipping help/tagsets.zip.\n",
      "       | Downloading package snowball_data to /root/nltk_data...\n",
      "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
      "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "       | Downloading package word2vec_sample to /root/nltk_data...\n",
      "       |   Unzipping models/word2vec_sample.zip.\n",
      "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
      "       | Downloading package mte_teip5 to /root/nltk_data...\n",
      "       |   Unzipping corpora/mte_teip5.zip.\n",
      "       | Downloading package averaged_perceptron_tagger to\n",
      "       |     /root/nltk_data...\n",
      "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "       | Downloading package averaged_perceptron_tagger_ru to\n",
      "       |     /root/nltk_data...\n",
      "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
      "       | Downloading package perluniprops to /root/nltk_data...\n",
      "       |   Unzipping misc/perluniprops.zip.\n",
      "       | Downloading package nonbreaking_prefixes to\n",
      "       |     /root/nltk_data...\n",
      "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "       | Downloading package vader_lexicon to /root/nltk_data...\n",
      "       | Downloading package porter_test to /root/nltk_data...\n",
      "       |   Unzipping stemmers/porter_test.zip.\n",
      "       | Downloading package wmt15_eval to /root/nltk_data...\n",
      "       |   Unzipping models/wmt15_eval.zip.\n",
      "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
      "       |   Unzipping misc/mwa_ppdb.zip.\n",
      "       | \n",
      "     Done downloading collection all\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()   # Run this the first time after installing nltk package. It will open a GUI. Select all and hit Download.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qjhN0UlgYyEW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WysE9R03Y1D3"
   },
   "outputs": [],
   "source": [
    "# 1.\tLoad the dataset (5 points) \n",
    "  # a.\tTip: As the dataset is large, use fewer rows. Check what is working well on your machine and decide accordingly\n",
    "blog_text = pd.read_csv('blogtext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6_tCPU-SZTNh"
   },
   "outputs": [],
   "source": [
    "# Taking first 50000 rows\n",
    "blog_text = blog_text.head(1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2hv3zTcYZgGR",
    "outputId": "cd3f500e-c01b-4bb9-d589-90464edeb42c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r7OvIzDuZuSF"
   },
   "outputs": [],
   "source": [
    "# 2. Preprocess rows of the “text” column (7.5 points)\n",
    "  # a.\tRemove unwanted characters \n",
    "  # b.\tConvert text to lowercase \n",
    "  # c.\tRemove unwanted spaces \n",
    "  # d.\tRemove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "blog_text['text'] = blog_text['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KRwZZDRudwrm"
   },
   "outputs": [],
   "source": [
    "blog_text['text'] = blog_text['text'].apply(lambda x: remove_spaces(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FLwPdyzNiA-l"
   },
   "outputs": [],
   "source": [
    "blog_text['text'] = blog_text['text'].apply(lambda x: remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1zM2631AicQu"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "blog_text['text'] = blog_text['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o9l78WID1Hq_"
   },
   "outputs": [],
   "source": [
    "blog_text['text'] = blog_text['text'].apply(lambda x: remove_non_english_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ty4fJwFqg4oP"
   },
   "outputs": [],
   "source": [
    "import string \n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "def remove_spaces(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def remove_punctuation(text): \n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    return text.translate(translator) \n",
    "\n",
    "def remove_non_english_words(text):\n",
    "\n",
    "  return \" \".join(w for w in nltk.wordpunct_tokenize(text) \\\n",
    "\n",
    "         if w.lower() in words )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "CjdeP5mIdcJ9",
    "outputId": "298d82d1-e55b-481a-8595-f3b5e5938fce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hour long conversation hair guy never met life hair blue kept thinking another one psychotic ate chocolate ever thought possible ha ha record crazy oh wow deep breath laugh sigh'"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_text['text'][5353]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t7TDGFxPe0p8"
   },
   "outputs": [],
   "source": [
    "# blog_text['text1'][5353]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "i7TBFyWMk6Pv",
    "outputId": "36aa3b80-bf95-4fc0-d037-02abf78e97e2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>found wait untill team leader</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>team van mail mail mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>het van build subject build humorous date orga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>testing testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>thanks capture show cool links pop audio video...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  ...                                               text\n",
       "0  2059027  ...                      found wait untill team leader\n",
       "1  2059027  ...                            team van mail mail mail\n",
       "2  2059027  ...  het van build subject build humorous date orga...\n",
       "3  2059027  ...                                    testing testing\n",
       "4  3581210  ...  thanks capture show cool links pop audio video...\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.\tAs we want to make this into a multi-label classification problem, you are required to merge all the label columns together, so that we have all the labels together for a particular sentence (7.5 points) \n",
    "  # a.\tLabel columns to merge: “gender”, “age”, “topic”, “sign” \n",
    "  # b.\tAfter completing the previous step, there should be only two columns in your data frame i.e. “text” and “labels” as shown in the below image\n",
    "\n",
    "blog_text.head()\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gEwwZMc7micg"
   },
   "outputs": [],
   "source": [
    "blog_text['labels'] = blog_text[['gender','age','topic','sign']].apply(lambda x: creating_label(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0GkFThvArYpe"
   },
   "outputs": [],
   "source": [
    "def creating_label(text): \n",
    "    column_list = []\n",
    "    column_list.append(text['gender'])\n",
    "    column_list.append(text['age'])\n",
    "    column_list.append(text['topic'])\n",
    "    column_list.append(text['sign'])\n",
    "    return column_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "0z8AlPfVp_eN",
    "outputId": "c2802d41-1b92-40f3-dcc1-34cd549e1240"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>found wait untill team leader</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>team van mail mail mail</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>het van build subject build humorous date orga...</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>testing testing</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>thanks capture show cool links pop audio video...</td>\n",
       "      <td>[male, 33, InvestmentBanking, Aquarius]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  ...                                   labels\n",
       "0  2059027  ...                 [male, 15, Student, Leo]\n",
       "1  2059027  ...                 [male, 15, Student, Leo]\n",
       "2  2059027  ...                 [male, 15, Student, Leo]\n",
       "3  2059027  ...                 [male, 15, Student, Leo]\n",
       "4  3581210  ...  [male, 33, InvestmentBanking, Aquarius]\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lr3Va5oQtI_t"
   },
   "outputs": [],
   "source": [
    "blog_text = blog_text.drop(['gender','age','topic','sign','id','date'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "mhzgccN0tPdO",
    "outputId": "de7bc747-064b-4fd8-cc41-0eb53f2f1822"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>found wait untill team leader</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>team van mail mail mail</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>het van build subject build humorous date orga...</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>testing testing</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thanks capture show cool links pop audio video...</td>\n",
       "      <td>[male, 33, InvestmentBanking, Aquarius]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                                   labels\n",
       "0                      found wait untill team leader                 [male, 15, Student, Leo]\n",
       "1                            team van mail mail mail                 [male, 15, Student, Leo]\n",
       "2  het van build subject build humorous date orga...                 [male, 15, Student, Leo]\n",
       "3                                    testing testing                 [male, 15, Student, Leo]\n",
       "4  thanks capture show cool links pop audio video...  [male, 33, InvestmentBanking, Aquarius]"
      ]
     },
     "execution_count": 164,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "blog_text.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ijaUMYbAteNW"
   },
   "outputs": [],
   "source": [
    "# 4. Separate features and labels, and split the data into training and testing (5 points)\n",
    "\n",
    "# define X and y\n",
    "X = blog_text['text']\n",
    "y = blog_text['labels']\n",
    "\n",
    "# split the new DataFrame into training and testing sets [Default test size = 25%]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "z5XL9iFQwdxa",
    "outputId": "15ec8ca3-0bc9-42ff-9b98-7b77371c9dd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750,)\n",
      "(250,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PL2G5NUyutD5"
   },
   "outputs": [],
   "source": [
    "# 5.\tVectorize the features (5 points) \n",
    "#   a.\tCreate a Bag of Words using count vectorizer \n",
    "      # i.\tUse ngram_range=(1, 2) \n",
    "      # ii.\tVectorize training and testing features \n",
    "  # b.\tPrint the term-document matrix\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MiM1tQE5w0AK",
    "outputId": "cfafbfdd-01c3-4cb8-a482-9e9942b51227"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750, 72476)"
      ]
     },
     "execution_count": 194,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "-MVLyIk0w7Im",
    "outputId": "0ef15f5c-a8bc-481c-d4dc-1bf8b66d7df4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yr happening', 'yr hovering', 'yr last', 'yr new', 'yr oh', 'yr old', 'yr principal', 'yr pro', 'yr projection', 'yr really', 'yr since', 'yr theory', 'yr thing', 'yr toiletry', 'yr used', 'yr went', 'yr would', 'yuan', 'yuan day', 'yucca', 'yucca improve', 'yuck', 'yuck anyway', 'yummy', 'yummy colors', 'yummy food', 'yummy recipe', 'zero', 'zero help', 'zero quite', 'zest', 'zest close', 'zimbabwe', 'zimbabwe republic', 'zipping', 'zipping past', 'zodiac', 'zodiac sign', 'zodiac similar', 'zombie', 'zombie right', 'zone', 'zone beyond', 'zone hope', 'zone large', 'zone one', 'zone went', 'zone work', 'zoo', 'zoo face']\n"
     ]
    }
   ],
   "source": [
    "# last 50 features\n",
    "print(vect.get_feature_names()[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fTIOYKeY5CsZ"
   },
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(X_train_dtm.head(10).toarray(), columns=vect.get_feature_names()[10:])\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6DGPV9ag95dm"
   },
   "outputs": [],
   "source": [
    "# 6.\tCreate a dictionary to get the count of every label i.e. the key will be label name and value will be the total count of the label. Check below image for reference (5 points)\n",
    "\n",
    "\n",
    "df = pd.Series(sum([item for item in y_train], [])).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VeGTLjyy-4mX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "4iXNRDXxjOQt",
    "outputId": "7c8b459c-a149-4128-d6b3-86a9253358e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{14: 50,\n",
       " 15: 51,\n",
       " 17: 108,\n",
       " 23: 55,\n",
       " 24: 248,\n",
       " 25: 45,\n",
       " 26: 28,\n",
       " 27: 62,\n",
       " 33: 61,\n",
       " 34: 6,\n",
       " 37: 12,\n",
       " 41: 11,\n",
       " 44: 1,\n",
       " 45: 12,\n",
       " 'Aquarius': 161,\n",
       " 'Aries': 33,\n",
       " 'Arts': 1,\n",
       " 'Banking': 8,\n",
       " 'BusinessServices': 16,\n",
       " 'Cancer': 56,\n",
       " 'Capricorn': 64,\n",
       " 'Communications-Media': 11,\n",
       " 'Education': 62,\n",
       " 'Engineering': 84,\n",
       " 'Gemini': 11,\n",
       " 'InvestmentBanking': 53,\n",
       " 'Leo': 30,\n",
       " 'Libra': 127,\n",
       " 'Non-Profit': 37,\n",
       " 'Pisces': 1,\n",
       " 'Sagittarius': 75,\n",
       " 'Science': 19,\n",
       " 'Scorpio': 186,\n",
       " 'Sports-Recreation': 62,\n",
       " 'Student': 109,\n",
       " 'Taurus': 4,\n",
       " 'Virgo': 2,\n",
       " 'female': 320,\n",
       " 'indUnk': 288,\n",
       " 'male': 430}"
      ]
     },
     "execution_count": 220,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_dict() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PIxF180O_Waa"
   },
   "outputs": [],
   "source": [
    "# 7.Transform the labels - (7.5 points) As we have noticed before, in this task each example can have multiple tags. To deal with such kind of prediction, we need to transform labels in a binary form and the prediction will be a mask of 0s and 1s. For this purpose, it is convenient to use MultiLabelBinarizer from sklearn \n",
    "#   a.\tConvert your train and test labels using MultiLabelBinarizer\n",
    "\n",
    "j = 0\n",
    "for x in y_test:\n",
    "  ind = y_test.index[j]\n",
    "  y_test[ind] = [str(i) for i in x]\n",
    "  j = j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0PgdZ36Tas47"
   },
   "outputs": [],
   "source": [
    "j = 0\n",
    "for x in y_train:\n",
    "  ind = y_train.index[j]\n",
    "  y_train[ind] = [str(i) for i in x]\n",
    "  j = j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zfn4-P6qavyk"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb_train = MultiLabelBinarizer()\n",
    "mlb_train_data = mlb_train.fit_transform(y_train.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e75l17jErIJq"
   },
   "outputs": [],
   "source": [
    "\n",
    "mlb_test = mlb_train.transform(y_test.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "lZnaPrCpAAnJ",
    "outputId": "20c2ba0a-7edf-4b05-dbd0-4669f07a0883"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['14', '15', '17', '23', '24', '25', '26', '27', '33', '34', '37',\n",
       "       '41', '44', '45', 'Aquarius', 'Aries', 'Arts', 'Banking',\n",
       "       'BusinessServices', 'Cancer', 'Capricorn', 'Communications-Media',\n",
       "       'Education', 'Engineering', 'Gemini', 'InvestmentBanking', 'Leo',\n",
       "       'Libra', 'Non-Profit', 'Pisces', 'Sagittarius', 'Science',\n",
       "       'Scorpio', 'Sports-Recreation', 'Student', 'Taurus', 'Virgo',\n",
       "       'female', 'indUnk', 'male'], dtype=object)"
      ]
     },
     "execution_count": 226,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb_train.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RQ4uD5QLZFnl",
    "outputId": "b754bdc1-46b6-4a16-9132-222e9036a4a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 40)"
      ]
     },
     "execution_count": 227,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PXosG6mJGnDt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wbCfB9UcGslN",
    "outputId": "21d3109a-b9d9-48e4-d11a-d8a9f7bcf5be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750,)"
      ]
     },
     "execution_count": 205,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ejcp84EqE13U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Yp-cwptcCk_v",
    "outputId": "0a24a699-abcf-48b7-f9a9-84c18d938cfc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['14', '15', '17', '23', '24', '25', '26', '27', '33', '34', '37',\n",
       "       '41', '44', '45', 'Aquarius', 'Aries', 'Arts', 'Banking',\n",
       "       'BusinessServices', 'Cancer', 'Capricorn', 'Communications-Media',\n",
       "       'Education', 'Engineering', 'Gemini', 'InvestmentBanking', 'Leo',\n",
       "       'Libra', 'Non-Profit', 'Pisces', 'Sagittarius', 'Science',\n",
       "       'Scorpio', 'Sports-Recreation', 'Student', 'Taurus', 'Virgo',\n",
       "       'female', 'indUnk', 'male'], dtype=object)"
      ]
     },
     "execution_count": 228,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb_train.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ozKNVJpZavU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "92hg_3SkDhYr"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 8.\t Choose a classifier - (5 points) In this task, we suggest using the One-vs-Rest approach, which is implemented in OneVsRestClassifier class. In this approach k classifiers (= number of tags) are trained. As a basic classifier, use LogisticRegression. It is one of the simplest methods, but often it performs good enough in text classification tasks. It might take some time because the number of classifiers to train is large. \n",
    "#       a.\tUse a linear classifier of your choice, wrap it up in OneVsRestClassifier to train it on every label \n",
    "#       b.\tAs One-vs-Rest approach might not have been discussed in the sessions, we are providing you the code for that\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(solver = 'lbfgs')\n",
    "clf = OneVsRestClassifier(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "YQ5QAQkaOrPa",
    "outputId": "58003262-bbd0-4b14-cc78-e6f5faa21416"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='warn',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=None,\n",
       "                                                 solver='lbfgs', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": 230,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9.\tFit the classifier, make predictions and get the accuracy (5 points) \n",
    "#     a.\tPrint the following \n",
    "#       i.\tAccuracy score \n",
    "#       ii.\tF1 score \n",
    "#       iii.\tAverage precision score \n",
    "#       iv.\tAverage recall score v.\tTip: Make sure you are familiar with all of them. How would you expect the things to work for the multi-label scenario? Read about micro/macro/weighted averaging\n",
    "\n",
    "\n",
    "clf.fit(X_train_dtm,mlb_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HzjctlFQZ7Os",
    "outputId": "57a23cef-7acb-4283-fbdc-b8e674a6bd87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 72476)"
      ]
     },
     "execution_count": 231,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7aRdkTaVPDuq"
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9zH_8KcFXvFL",
    "outputId": "a0a2e83e-252f-49d8-b23e-4e795a0fe678"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 40)"
      ]
     },
     "execution_count": 233,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5_xoPhaQYA8J",
    "outputId": "756f20d3-f186-4ed4-ff04-6d2e7eb105e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.172\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(mlb_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sBCZBXl5YcyB",
    "outputId": "c2079e68-2d80-4a19-a825-43be51778dc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27173211695640764\n"
     ]
    }
   ],
   "source": [
    "print(metrics.f1_score(mlb_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ETBLjoA2YfXb",
    "outputId": "125ddc12-da53-4a09-d61b-c39bd12b0e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5206997313873186\n"
     ]
    }
   ],
   "source": [
    "print(metrics.precision_score(mlb_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WXC_TWBRcAKs",
    "outputId": "84cca71a-4c4b-4910-8a02-cd2802fad626"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2037730228308003\n"
     ]
    }
   ],
   "source": [
    "print(metrics.recall_score(mlb_test, y_pred, average='macro'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "nltk_project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
